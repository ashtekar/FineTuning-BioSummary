{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI DPO Fine-tuning\n",
        "\n",
        "This notebook implements the fine-tuning process for OpenAI 4o-mini using DPO (Direct Preference Optimization) with your preference data.\n",
        "\n",
        "## Process Overview\n",
        "1. **Setup** OpenAI client and authentication\n",
        "2. **Extract** transformed data from `dpo_training_data` table\n",
        "3. **Format** data for OpenAI DPO training\n",
        "4. **Upload** training file to OpenAI\n",
        "5. **Start** DPO fine-tuning job\n",
        "6. **Monitor** training progress\n",
        "7. **Deploy** the fine-tuned model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# OpenAI\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Database\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Environment\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('.env.local')\n",
        "\n",
        "print(\"âœ… All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize OpenAI Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… OpenAI client initialized successfully\n",
            "âœ… Connection verified - 86 models available\n"
          ]
        }
      ],
      "source": [
        "# Initialize OpenAI client\n",
        "def initialize_openai():\n",
        "    \"\"\"Initialize OpenAI client using environment variables\"\"\"\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    \n",
        "    if not api_key:\n",
        "        raise ValueError(\"Missing OPENAI_API_KEY in .env.local\")\n",
        "    \n",
        "    client = OpenAI(api_key=api_key)\n",
        "    return client\n",
        "\n",
        "# Initialize OpenAI connection\n",
        "try:\n",
        "    openai_client = initialize_openai()\n",
        "    print(\"âœ… OpenAI client initialized successfully\")\n",
        "    \n",
        "    # Test the connection\n",
        "    models = openai_client.models.list()\n",
        "    print(f\"âœ… Connection verified - {len(models.data)} models available\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing OpenAI: {e}\")\n",
        "    print(\"Please check your OPENAI_API_KEY in .env.local\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Supabase Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Supabase client initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize Supabase client\n",
        "def initialize_supabase():\n",
        "    \"\"\"Initialize Supabase client using environment variables\"\"\"\n",
        "    url = os.getenv(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
        "    key = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
        "    \n",
        "    if not url or not key:\n",
        "        raise ValueError(\"Missing Supabase credentials in .env.local\")\n",
        "    \n",
        "    supabase: Client = create_client(url, key)\n",
        "    return supabase\n",
        "\n",
        "# Initialize Supabase connection\n",
        "try:\n",
        "    supabase = initialize_supabase()\n",
        "    print(\"âœ… Supabase client initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing Supabase: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract DPO Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Extracting DPO training data...\n",
            "âœ… Extracted 46 DPO training records\n",
            "\n",
            "ğŸ“ Sample DPO training record:\n",
            "  uuid: 8e99580f-45ee-432d-93c2-55200a6a67f4\n",
            "  input_message: Jun 28, 2018 ... Engineering CRISPR/Cpf1 with tRNA promotes genome ... molecules enhance CRISPR/Cas9... (length: 160)\n",
            "  preferred_output: How to create state-of-the-art genetic model systems: strategies for â€¦\n",
            "This article outlines practic...\n",
            "  non_preferred_output: Summary of Synthetic Biology Article\n",
            "\n",
            "\n",
            "Summary of \"How to create state-of-the-art genetic model syst...\n",
            "  created_at: 2025-09-12T22:28:49.699077+00:00\n"
          ]
        }
      ],
      "source": [
        "# Extract DPO training data from Supabase\n",
        "def extract_dpo_training_data(limit=None):\n",
        "    \"\"\"\n",
        "    Extract DPO training data from the dpo_training_data table\n",
        "    \n",
        "    Args:\n",
        "        limit: Optional limit for number of records to extract\n",
        "    \n",
        "    Returns:\n",
        "        List of DPO training records\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query = supabase.table('dpo_training_data').select('*')\n",
        "        \n",
        "        if limit:\n",
        "            query = query.limit(limit)\n",
        "        \n",
        "        response = query.execute()\n",
        "        \n",
        "        if response.data:\n",
        "            print(f\"âœ… Extracted {len(response.data)} DPO training records\")\n",
        "            return response.data\n",
        "        else:\n",
        "            print(\"âš ï¸ No data found in dpo_training_data table\")\n",
        "            print(\"Please run the ETL pipeline first to populate the table\")\n",
        "            return []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error extracting DPO training data: {e}\")\n",
        "        return []\n",
        "\n",
        "# Extract the data\n",
        "print(\"ğŸ“Š Extracting DPO training data...\")\n",
        "dpo_data = extract_dpo_training_data()\n",
        "\n",
        "# Show sample data structure\n",
        "if dpo_data:\n",
        "    print(f\"\\nğŸ“ Sample DPO training record:\")\n",
        "    sample = dpo_data[0]\n",
        "    for key, value in sample.items():\n",
        "        if key == 'input_message':\n",
        "            print(f\"  {key}: {value[:100]}... (length: {len(value)})\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
        "else:\n",
        "    print(\"âŒ No DPO training data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Format Data for OpenAI DPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Formatting data for OpenAI DPO fine-tuning...\n",
            "âœ… Formatted 46 training examples\n",
            "ğŸ“Š Training pairs: 46\n",
            "âœ… Training file saved: TrainingData/dpo_training_data_20250912_185907.jsonl\n",
            "ğŸ“Š Total examples: 46\n",
            "ğŸ“ Directory: /Users/ashtekar/SourceCode/FineTuning-BioSummary/TrainingData\n",
            "\n",
            "ğŸ“ Sample DPO training example:\n",
            "{\n",
            "  \"input\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are an expert science educator who specializes in making complex scientific concepts accessible to college students. Your audience is a College sophomore with a good foundation in basic biology. Create a concise summary of this synthetic biology article with the following requirements:\\n\\n1. Use simple, clear language\\n2. Explain complex terms when they appear\\n3. Focus on the main findings and their impor...\n",
            "\n",
            "ğŸ” DPO format structure:\n",
            "  - input.messages: Contains system + user messages\n",
            "  - input.tools: Empty array []\n",
            "  - input.parallel_tool_calls: True\n",
            "  - preferred_output: Array with assistant message\n",
            "  - non_preferred_output: Array with assistant message\n",
            "\n",
            "ğŸ“ All training files:\n",
            "ğŸ“ Found 1 training file(s) in TrainingData:\n",
            "  1. dpo_training_data_20250912_185907.jsonl (177,280 bytes)\n"
          ]
        }
      ],
      "source": [
        "# Format data for OpenAI DPO training\n",
        "# \n",
        "# IMPORTANT: Differences from standard fine-tuning:\n",
        "# 1. We're using OpenAI SDK (openai_client) - YES, correct\n",
        "# 2. Hyperparameters: DPO uses \"n_epochs\": \"auto\" (not learning_rate_multiplier)\n",
        "# 3. Data format: We only need preferred responses, not both preferred/rejected\n",
        "# 4. OpenAI DPO handles preference optimization internally\n",
        "def format_dpo_training_data(dpo_records):\n",
        "    \"\"\"\n",
        "    Format DPO training data for OpenAI fine-tuning\n",
        "    \n",
        "    Creates the exact format required by OpenAI's DPO fine-tuning API.\n",
        "    Each JSONL line contains BOTH preferred and non-preferred outputs.\n",
        "    \n",
        "    Args:\n",
        "        dpo_records: List of DPO training records from Supabase\n",
        "    \n",
        "    Returns:\n",
        "        List of formatted training examples for OpenAI DPO\n",
        "    \"\"\"\n",
        "    formatted_examples = []\n",
        "    \n",
        "    for record in dpo_records:\n",
        "        # Create DPO training example with BOTH preferred and non-preferred outputs\n",
        "        dpo_example = {\n",
        "            \"input\": {\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an expert science educator who specializes in making complex scientific concepts accessible to college students. Your audience is a College sophomore with a good foundation in basic biology. Create a concise summary of this synthetic biology article with the following requirements:\\n\\n1. Use simple, clear language\\n2. Explain complex terms when they appear\\n3. Focus on the main findings and their importance\\n4. Make it engaging and interesting\\n5. Keep it concise (2-3 paragraphs)\\n6. Content of your response will be used in an email newsletter\\n\\nPlease provide a simplified explanation that maintains scientific accuracy while being accessible to a college sophomore.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": record['input_message']\n",
        "                    }\n",
        "                ],\n",
        "                \"tools\": [],\n",
        "                \"parallel_tool_calls\": True\n",
        "            },\n",
        "            \"preferred_output\": [\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": record['preferred_output']\n",
        "                }\n",
        "            ],\n",
        "            \"non_preferred_output\": [\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": record['non_preferred_output']\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        formatted_examples.append(dpo_example)\n",
        "    \n",
        "    return formatted_examples\n",
        "\n",
        "def save_training_file(formatted_examples, filename=None):\n",
        "    \"\"\"\n",
        "    Save formatted examples to JSONL file for OpenAI upload\n",
        "    \n",
        "    Args:\n",
        "        formatted_examples: List of formatted training examples\n",
        "        filename: Optional custom filename (if None, auto-generates with timestamp)\n",
        "    \n",
        "    Returns:\n",
        "        Path to saved file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import os\n",
        "        from datetime import datetime\n",
        "        \n",
        "        # Create TrainingData directory if it doesn't exist\n",
        "        training_dir = \"TrainingData\"\n",
        "        os.makedirs(training_dir, exist_ok=True)\n",
        "        \n",
        "        # Generate filename with timestamp if not provided\n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"dpo_training_data_{timestamp}.jsonl\"\n",
        "        \n",
        "        # Ensure filename has .jsonl extension\n",
        "        if not filename.endswith('.jsonl'):\n",
        "            filename += '.jsonl'\n",
        "        \n",
        "        # Create full path\n",
        "        file_path = os.path.join(training_dir, filename)\n",
        "        \n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            for example in formatted_examples:\n",
        "                f.write(json.dumps(example) + '\\n')\n",
        "        \n",
        "        print(f\"âœ… Training file saved: {file_path}\")\n",
        "        print(f\"ğŸ“Š Total examples: {len(formatted_examples)}\")\n",
        "        print(f\"ğŸ“ Directory: {os.path.abspath(training_dir)}\")\n",
        "        return file_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error saving training file: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_training_files():\n",
        "    \"\"\"\n",
        "    List all existing training files in the TrainingData directory\n",
        "    \n",
        "    Returns:\n",
        "        List of training file paths\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import glob\n",
        "    \n",
        "    training_dir = \"TrainingData\"\n",
        "    if not os.path.exists(training_dir):\n",
        "        print(f\"ğŸ“ TrainingData directory doesn't exist yet\")\n",
        "        return []\n",
        "    \n",
        "    # Find all .jsonl files in TrainingData directory\n",
        "    pattern = os.path.join(training_dir, \"*.jsonl\")\n",
        "    training_files = glob.glob(pattern)\n",
        "    \n",
        "    if training_files:\n",
        "        print(f\"ğŸ“ Found {len(training_files)} training file(s) in TrainingData:\")\n",
        "        for i, file_path in enumerate(sorted(training_files), 1):\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"  {i}. {os.path.basename(file_path)} ({file_size:,} bytes)\")\n",
        "    else:\n",
        "        print(f\"ğŸ“ No training files found in TrainingData directory\")\n",
        "    \n",
        "    return training_files\n",
        "\n",
        "# Format the data for OpenAI DPO fine-tuning\n",
        "if dpo_data:\n",
        "    print(\"ğŸ”„ Formatting data for OpenAI DPO fine-tuning...\")\n",
        "    formatted_examples = format_dpo_training_data(dpo_data)\n",
        "    \n",
        "    print(f\"âœ… Formatted {len(formatted_examples)} training examples\")\n",
        "    print(f\"ğŸ“Š Training pairs: {len(dpo_data)}\")\n",
        "    \n",
        "    # Save to file with timestamp\n",
        "    training_file = save_training_file(formatted_examples)\n",
        "    \n",
        "    # Show sample formatted example\n",
        "    if formatted_examples:\n",
        "        print(f\"\\nğŸ“ Sample DPO training example:\")\n",
        "        sample = formatted_examples[0]\n",
        "        print(json.dumps(sample, indent=2)[:500] + \"...\")\n",
        "        \n",
        "        print(f\"\\nğŸ” DPO format structure:\")\n",
        "        print(f\"  - input.messages: Contains system + user messages\")\n",
        "        print(f\"  - input.tools: Empty array []\")\n",
        "        print(f\"  - input.parallel_tool_calls: True\")\n",
        "        print(f\"  - preferred_output: Array with assistant message\")\n",
        "        print(f\"  - non_preferred_output: Array with assistant message\")\n",
        "        \n",
        "    # List all training files\n",
        "    print(f\"\\nğŸ“ All training files:\")\n",
        "    list_training_files()\n",
        "else:\n",
        "    print(\"âŒ No data to format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Checking existing training files...\n",
            "ğŸ“ Found 1 training file(s) in TrainingData:\n",
            "  1. dpo_training_data_20250912_185907.jsonl (177,280 bytes)\n"
          ]
        }
      ],
      "source": [
        "# List all existing training files in TrainingData directory\n",
        "print(\"ğŸ“ Checking existing training files...\")\n",
        "existing_files = list_training_files()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Upload Training File to OpenAI & start the fine tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Uploading training file and starting DPO fine-tuning...\n",
            "ğŸ“¤ Uploading training file: TrainingData/dpo_training_data_20250912_185907.jsonl\n",
            "âœ… Training file uploaded successfully\n",
            "ğŸ“‹ File ID: file-3APfFWrkPtdwFQqBpRXwsi\n",
            "ğŸš€ Starting DPO fine-tuning job...\n",
            "ğŸ“‹ Model: gpt-4.1-nano-2025-04-14\n",
            "âœ… DPO fine-tuning job started successfully\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: validating_files\n",
            "\n",
            "ğŸ‰ DPO fine-tuning job initiated!\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: validating_files\n",
            "\n",
            "â³ Training will take some time to complete...\n",
            "ğŸ“± You can monitor progress in the OpenAI dashboard or run the monitoring cell below\n"
          ]
        }
      ],
      "source": [
        "# Upload training file to OpenAI\n",
        "def upload_and_start_dpo_finetuning(file_path, model=\"gpt-4.1-nano-2025-04-14\"):\n",
        "    \"\"\"\n",
        "    Upload training file and start DPO fine-tuning job in one step\n",
        "    \n",
        "    Based on OpenAI documentation example:\n",
        "    https://platform.openai.com/docs/guides/direct-preference-optimization\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the JSONL training file\n",
        "        model: Base model to fine-tune (default: gpt-4o-mini)\n",
        "    \n",
        "    Returns:\n",
        "        Fine-tuning job object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ğŸ“¤ Uploading training file: {file_path}\")\n",
        "        \n",
        "        # Upload the training file\n",
        "        with open(file_path, 'rb') as f:\n",
        "            file_response = openai_client.files.create(\n",
        "                file=f,\n",
        "                purpose='fine-tune'\n",
        "            )\n",
        "        \n",
        "        file_id = file_response.id\n",
        "        print(f\"âœ… Training file uploaded successfully\")\n",
        "        print(f\"ğŸ“‹ File ID: {file_id}\")\n",
        "        \n",
        "        print(f\"ğŸš€ Starting DPO fine-tuning job...\")\n",
        "        print(f\"ğŸ“‹ Model: {model}\")\n",
        "        \n",
        "        # Start the DPO fine-tuning job using the new format\n",
        "        job_response = openai_client.fine_tuning.jobs.create(\n",
        "            training_file=file_id,\n",
        "            model=model,\n",
        "            method={\n",
        "                \"type\": \"dpo\",\n",
        "                \"dpo\": {\n",
        "                    \"hyperparameters\": {\"beta\": 0.1}\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        job_id = job_response.id\n",
        "        print(f\"âœ… DPO fine-tuning job started successfully\")\n",
        "        print(f\"ğŸ“‹ Job ID: {job_id}\")\n",
        "        print(f\"ğŸ“Š Status: {job_response.status}\")\n",
        "        \n",
        "        return job_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error uploading file and starting fine-tuning: {e}\")\n",
        "        return None\n",
        "\n",
        "# Upload training file and start DPO fine-tuning job\n",
        "if 'training_file' in locals() and training_file:\n",
        "    print(\"ğŸš€ Uploading training file and starting DPO fine-tuning...\")\n",
        "    finetuning_job = upload_and_start_dpo_finetuning(training_file)\n",
        "    \n",
        "    if finetuning_job:\n",
        "        print(f\"\\nğŸ‰ DPO fine-tuning job initiated!\")\n",
        "        print(f\"ğŸ“‹ Job ID: {finetuning_job.id}\")\n",
        "        print(f\"ğŸ“Š Status: {finetuning_job.status}\")\n",
        "        print(f\"\\nâ³ Training will take some time to complete...\")\n",
        "        print(f\"ğŸ“± You can monitor progress in the OpenAI dashboard or run the monitoring cell below\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to upload file and start fine-tuning job\")\n",
        "else:\n",
        "    print(\"âŒ No training file available to upload and start fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Enhanced DPO monitoring functions created!\n",
            "ğŸ“Š New metrics: Training Loss, Preference Accuracy, Cost Estimation, Validation Loss, Learning Rate\n",
            "ğŸš« Removed: Trained tokens, Completion time\n"
          ]
        }
      ],
      "source": [
        "# Enhanced DPO Training Metrics\n",
        "def calculate_training_metrics(response, start_time=None):\n",
        "    \"\"\"\n",
        "    Calculate enhanced DPO training metrics\n",
        "    \n",
        "    Args:\n",
        "        response: OpenAI fine-tuning job response\n",
        "        start_time: Job start time for duration calculation\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of calculated metrics\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "    \n",
        "    metrics = {}\n",
        "    \n",
        "    # Training Loss (DPO-specific)\n",
        "    if hasattr(response, 'training_loss'):\n",
        "        metrics['training_loss'] = response.training_loss\n",
        "    elif hasattr(response, 'result_files'):\n",
        "        # If training loss is in result files, we'd need to download and parse\n",
        "        metrics['training_loss'] = \"Available in result files\"\n",
        "    \n",
        "    # Preference Accuracy (DPO-specific)\n",
        "    if hasattr(response, 'preference_accuracy'):\n",
        "        metrics['preference_accuracy'] = response.preference_accuracy\n",
        "    else:\n",
        "        metrics['preference_accuracy'] = \"Calculated post-training\"\n",
        "    \n",
        "    # Cost Estimation\n",
        "    if hasattr(response, 'trained_tokens') and response.trained_tokens:\n",
        "        # OpenAI DPO pricing (approximate)\n",
        "        dpo_cost_per_1k_tokens = 0.008  # $8 per 1M tokens for DPO\n",
        "        estimated_cost = (response.trained_tokens / 1000) * dpo_cost_per_1k_tokens\n",
        "        metrics['estimated_cost'] = f\"${estimated_cost:.4f}\"\n",
        "    else:\n",
        "        metrics['estimated_cost'] = \"Calculating...\"\n",
        "    \n",
        "    # Validation Loss\n",
        "    if hasattr(response, 'validation_loss'):\n",
        "        metrics['validation_loss'] = response.validation_loss\n",
        "    else:\n",
        "        metrics['validation_loss'] = \"Not available during training\"\n",
        "    \n",
        "    # Preference Learning Rate\n",
        "    if hasattr(response, 'learning_rate'):\n",
        "        metrics['preference_learning_rate'] = response.learning_rate\n",
        "    elif hasattr(response, 'hyperparameters'):\n",
        "        metrics['preference_learning_rate'] = getattr(response.hyperparameters, 'learning_rate', 'Auto')\n",
        "    else:\n",
        "        metrics['preference_learning_rate'] = \"Auto (DPO default)\"\n",
        "    \n",
        "    # Training Duration\n",
        "    if start_time and hasattr(response, 'finished_at') and response.finished_at:\n",
        "        duration = response.finished_at - start_time\n",
        "        metrics['training_duration'] = f\"{duration:.0f} seconds\"\n",
        "    elif start_time:\n",
        "        current_time = time.time()\n",
        "        duration = current_time - start_time\n",
        "        metrics['training_duration'] = f\"{duration:.0f} seconds (ongoing)\"\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def monitor_finetuning_job(job_id, start_time=None):\n",
        "    \"\"\"\n",
        "    Monitor the progress of a DPO fine-tuning job with enhanced metrics\n",
        "    \n",
        "    Args:\n",
        "        job_id: ID of the fine-tuning job\n",
        "        start_time: Job start time for duration calculation\n",
        "    \n",
        "    Returns:\n",
        "        Job status and details\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ğŸ“Š Checking DPO fine-tuning job status...\")\n",
        "        print(f\"ğŸ“‹ Job ID: {job_id}\")\n",
        "        \n",
        "        # Get job status\n",
        "        response = openai_client.fine_tuning.jobs.retrieve(job_id)\n",
        "        \n",
        "        status = response.status\n",
        "        print(f\"ğŸ“Š Status: {status}\")\n",
        "        \n",
        "        # Calculate enhanced metrics\n",
        "        metrics = calculate_training_metrics(response, start_time)\n",
        "        \n",
        "        # Display metrics\n",
        "        print(f\"\\nğŸ“ˆ DPO Training Metrics:\")\n",
        "        print(f\"  ğŸ”¥ Training Loss: {metrics.get('training_loss', 'In progress...')}\")\n",
        "        print(f\"  ğŸ¯ Preference Accuracy: {metrics.get('preference_accuracy', 'Calculating...')}\")\n",
        "        print(f\"  ğŸ’° Estimated Cost: {metrics.get('estimated_cost', 'Calculating...')}\")\n",
        "        print(f\"  ğŸ“Š Validation Loss: {metrics.get('validation_loss', 'Not available')}\")\n",
        "        print(f\"  ğŸ“š Learning Rate: {metrics.get('preference_learning_rate', 'Auto')}\")\n",
        "        \n",
        "        if 'training_duration' in metrics:\n",
        "            print(f\"  â±ï¸ Training Duration: {metrics['training_duration']}\")\n",
        "        \n",
        "        if hasattr(response, 'fine_tuned_model') and response.fine_tuned_model:\n",
        "            print(f\"\\nâœ… Fine-tuned model: {response.fine_tuned_model}\")\n",
        "        \n",
        "        if hasattr(response, 'error') and response.error:\n",
        "            print(f\"\\nâŒ Error: {response.error}\")\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error monitoring job: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Enhanced DPO monitoring functions created!\")\n",
        "print(\"ğŸ“Š New metrics: Training Loss, Preference Accuracy, Cost Estimation, Validation Loss, Learning Rate\")\n",
        "print(\"ğŸš« Removed: Trained tokens, Completion time\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: validating_files\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "âŒ No fine-tuned model available yet\n",
            "Please wait for the fine-tuning job to complete\n"
          ]
        }
      ],
      "source": [
        "# Test the fine-tuned model\n",
        "def test_finetuned_model(model_name, test_prompt=\"Please summarize this article about machine learning.\"):\n",
        "    \"\"\"\n",
        "    Test the fine-tuned model with a sample prompt\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the fine-tuned model\n",
        "        test_prompt: Test prompt to send to the model\n",
        "    \n",
        "    Returns:\n",
        "        Model response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ğŸ§ª Testing fine-tuned model: {model_name}\")\n",
        "        print(f\"ğŸ“ Test prompt: {test_prompt}\")\n",
        "        \n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": test_prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        model_response = response.choices[0].message.content\n",
        "        print(f\"âœ… Model response:\")\n",
        "        print(f\"{model_response}\")\n",
        "        \n",
        "        return model_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error testing model: {e}\")\n",
        "        return None\n",
        "\n",
        "def compare_models(original_model, finetuned_model, test_prompt):\n",
        "    \"\"\"\n",
        "    Compare responses from original and fine-tuned models\n",
        "    \n",
        "    Args:\n",
        "        original_model: Original model name (e.g., 'gpt-4o-mini')\n",
        "        finetuned_model: Fine-tuned model name\n",
        "        test_prompt: Test prompt for comparison\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ” Comparing model responses...\")\n",
        "    print(f\"ğŸ“ Test prompt: {test_prompt}\")\n",
        "    \n",
        "    # Test original model\n",
        "    print(f\"\\nğŸ¤– Original Model ({original_model}):\")\n",
        "    original_response = test_finetuned_model(original_model, test_prompt)\n",
        "    \n",
        "    # Test fine-tuned model\n",
        "    print(f\"\\nğŸ¯ Fine-tuned Model ({finetuned_model}):\")\n",
        "    finetuned_response = test_finetuned_model(finetuned_model, test_prompt)\n",
        "    \n",
        "    return {\n",
        "        'original': original_response,\n",
        "        'finetuned': finetuned_response\n",
        "    }\n",
        "\n",
        "# Test the model (run this after fine-tuning completes)\n",
        "if 'finetuning_job' in locals() and finetuning_job:\n",
        "    # Get the fine-tuned model name from the job\n",
        "    job_status = monitor_finetuning_job(finetuning_job.id)\n",
        "    \n",
        "    if hasattr(job_status, 'fine_tuned_model') and job_status.fine_tuned_model:\n",
        "        model_name = job_status.fine_tuned_model\n",
        "        print(f\"\\nğŸ¯ Testing fine-tuned model: {model_name}\")\n",
        "        \n",
        "        # Test with a sample prompt (formatted as user message)\n",
        "        test_prompt = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"You are an expert science educator who specializes in making complex scientific concepts accessible to college students. Your audience is a College sophomore with a good foundation in basic biology. Create a concise summary of this synthetic biology article with the following requirements:\\n\\n1. Use simple, clear language\\n2. Explain complex terms when they appear\\n3. Focus on the main findings and their importance\\n4. Make it engaging and interesting\\n5. Keep it concise (2-3 paragraphs)\\n6. Content of your response will be used in an email newsletter\\n\\nPlease provide a simplified explanation that maintains scientific accuracy while being accessible to a college sophomore.\"\n",
        "        }\n",
        "        \n",
        "        # Test the model with proper message format\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[test_prompt],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… Fine-tuned model response:\")\n",
        "        print(f\"{response.choices[0].message.content}\")\n",
        "        \n",
        "        print(f\"\\nğŸ” To compare with original model, run:\")\n",
        "        print(f\"compare_models('gpt-4o-mini', '{model_name}', 'Your test prompt here')\")\n",
        "    else:\n",
        "        print(\"âŒ No fine-tuned model available yet\")\n",
        "        print(\"Please wait for the fine-tuning job to complete\")\n",
        "else:\n",
        "    print(\"âŒ No fine-tuning job to test\")\n",
        "    print(\"Please run the previous cells to start and complete a fine-tuning job\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Current DPO job status with enhanced metrics:\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "\n",
            "ğŸ”„ To wait for completion with monitoring, run:\n",
            "wait_for_completion('ftjob-ZQWn425zo10RD80XraLFv1Ms')\n"
          ]
        }
      ],
      "source": [
        "# Use monitoring for DPO fine-tuning jobs\n",
        "if 'finetuning_job' in locals() and finetuning_job:\n",
        "    job_id = finetuning_job.id\n",
        "    print(\"ğŸ“Š Current DPO job status with enhanced metrics:\")\n",
        "    monitor_finetuning_job(job_id)\n",
        "    \n",
        "    print(f\"\\nğŸ”„ To wait for completion with monitoring, run:\")\n",
        "    print(f\"wait_for_completion('{job_id}')\")\n",
        "else:\n",
        "    print(\"âŒ No fine-tuning job to monitor\")\n",
        "    print(\"Please run the previous cells to start a fine-tuning job first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Enhanced wait_for_completion function created!\n",
            "ğŸ“Š Uses monitor_finetuning_job() with DPO-specific metrics\n"
          ]
        }
      ],
      "source": [
        "# Wait for completion with DPO monitoring\n",
        "def wait_for_completion(job_id, check_interval=60, start_time=None):\n",
        "    \"\"\"\n",
        "    Wait for DPO fine-tuning job to complete with enhanced monitoring\n",
        "    \n",
        "    Args:\n",
        "        job_id: ID of the fine-tuning job\n",
        "        check_interval: Seconds between status checks\n",
        "        start_time: Job start time for duration calculation\n",
        "    \"\"\"\n",
        "    print(f\"â³ Waiting for DPO fine-tuning job to complete...\")\n",
        "    print(f\"ğŸ“‹ Job ID: {job_id}\")\n",
        "    print(f\"ğŸ”„ Checking every {check_interval} seconds with enhanced metrics\")\n",
        "    \n",
        "    while True:\n",
        "        job_status = monitor_finetuning_job(job_id, start_time)\n",
        "        \n",
        "        if not job_status:\n",
        "            break\n",
        "            \n",
        "        if job_status.status == 'succeeded':\n",
        "            print(f\"\\nğŸ‰ DPO Fine-tuning completed successfully!\")\n",
        "            if hasattr(job_status, 'fine_tuned_model'):\n",
        "                print(f\"âœ… Fine-tuned model: {job_status.fine_tuned_model}\")\n",
        "            break\n",
        "        elif job_status.status == 'failed':\n",
        "            print(f\"\\nâŒ DPO Fine-tuning failed!\")\n",
        "            if hasattr(job_status, 'error'):\n",
        "                print(f\"Error: {job_status.error}\")\n",
        "            break\n",
        "        elif job_status.status in ['cancelled', 'cancelling']:\n",
        "            print(f\"\\nâš ï¸ DPO Fine-tuning was cancelled\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"â³ Status: {job_status.status} - waiting...\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "print(\"âœ… Enhanced wait_for_completion function created!\")\n",
        "print(\"ğŸ“Š Uses monitor_finetuning_job() with DPO-specific metrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Waiting for DPO fine-tuning job to complete...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ”„ Checking every 60 seconds with enhanced metrics\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: running\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: Calculating...\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "â³ Status: running - waiting...\n",
            "ğŸ“Š Checking DPO fine-tuning job status...\n",
            "ğŸ“‹ Job ID: ftjob-ZQWn425zo10RD80XraLFv1Ms\n",
            "ğŸ“Š Status: succeeded\n",
            "\n",
            "ğŸ“ˆ DPO Training Metrics:\n",
            "  ğŸ”¥ Training Loss: Available in result files\n",
            "  ğŸ¯ Preference Accuracy: Calculated post-training\n",
            "  ğŸ’° Estimated Cost: $1.4113\n",
            "  ğŸ“Š Validation Loss: Not available during training\n",
            "  ğŸ“š Learning Rate: Auto\n",
            "\n",
            "âœ… Fine-tuned model: ft:gpt-4.1-nano-2025-04-14:personal::CFRUvxM1\n",
            "\n",
            "âŒ Error: Error(code=None, message=None, param=None)\n",
            "\n",
            "ğŸ‰ DPO Fine-tuning completed successfully!\n",
            "âœ… Fine-tuned model: ft:gpt-4.1-nano-2025-04-14:personal::CFRUvxM1\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: Function Usage Guide\n",
        "# ================================\n",
        "# \n",
        "# âœ… USE THESE DPO FUNCTIONS:\n",
        "# - monitor_finetuning_job(job_id, start_time=None)\n",
        "# - wait_for_completion(job_id, check_interval=60, start_time=None)\n",
        "# - calculate_training_metrics(response, start_time=None)\n",
        "#\n",
        "# ğŸ“Š DPO functions provide:\n",
        "# - Training Loss, Preference Accuracy, Cost Estimation\n",
        "# - Validation Loss, Learning Rate, Training Duration\n",
        "# - DPO-specific metrics and better insights\n",
        "\n",
        "wait_for_completion('ftjob-ZQWn425zo10RD80XraLFv1Ms')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
